<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Transformer pytorch</title>
    <url>/2022/08/01/Transformer-pytorch/</url>
    <content><![CDATA[<h3 id="Transformer概览"><a href="#Transformer概览" class="headerlink" title="Transformer概览"></a>Transformer概览</h3><p><img src="/2022/08/01/Transformer-pytorch/901659325530_.pic_%E5%89%AF%E6%9C%AC.png" alt="901659325530_.pic"></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ======================================</span></span><br><span class="line"><span class="comment"># === Pytorch手写Transformer完整代码</span></span><br><span class="line"><span class="comment"># ======================================</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="comment"># device = &#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># transformer epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子</span></span><br><span class="line"><span class="comment"># 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度</span></span><br><span class="line"><span class="comment"># S: Symbol that shows starting of decoding input</span></span><br><span class="line"><span class="comment"># E: Symbol that shows starting of decoding output</span></span><br><span class="line"><span class="comment"># P: Symbol that will fill in blank sequence if current batch data size is short than time steps</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># 中文和英语的单词个数不要求相同</span></span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 好 朋 友 P&#x27;</span>, <span class="string">&#x27;S i have a good friend .&#x27;</span>, <span class="string">&#x27;i have a good friend . E&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 有 零 个 女 朋 友 P&#x27;</span>, <span class="string">&#x27;S i have zero girl friend .&#x27;</span>, <span class="string">&#x27;i have zero girl friend . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集（希望transformer能达到的效果）</span></span><br><span class="line"><span class="comment"># 输入：&quot;我 有 一 个 女 朋 友&quot;</span></span><br><span class="line"><span class="comment"># 输出：&quot;i have a girlfriend&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文和英语的单词要分开建立词库</span></span><br><span class="line"><span class="comment"># Padding Should be Zero</span></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;我&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;有&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;一&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;个&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;好&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;朋&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;友&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;零&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;女&#x27;</span>: <span class="number">9</span>&#125;</span><br><span class="line">src_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;have&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;good&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;friend&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;zero&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;girl&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line">src_len = <span class="number">8</span>  <span class="comment"># （源句子的长度）enc_input max sequence length</span></span><br><span class="line">tgt_len = <span class="number">7</span>  <span class="comment"># dec_input(=dec_output) max sequence length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transformer Parameters</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># Embedding Size 每个词被嵌入成几维词向量</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># FeedForward dimension (两次线性层中的隐藏层 512-&gt;2048-&gt;512，线性层是用来做特征提取的），当然最后会再接一个projection层</span></span><br><span class="line">d_k = d_v = <span class="number">64</span>  <span class="comment"># dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># number of Encoder of Decoder Layer（Block的个数）</span></span><br><span class="line">n_heads = <span class="number">8</span>  <span class="comment"># number of heads in Multi-Head Attention（有几个头）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============================================================================================</span></span><br><span class="line"><span class="comment"># 数据构建</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;把单词序列转换为数字序列&quot;&quot;&quot;</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [[src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]</span></span><br><span class="line">        dec_input = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]]  <span class="comment"># [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]</span></span><br><span class="line">        dec_output = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]</span></span><br><span class="line"></span><br><span class="line">        enc_inputs.extend(enc_input) <span class="comment"># [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]</span></span><br><span class="line">        dec_inputs.extend(dec_input) <span class="comment"># [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]</span></span><br><span class="line">        dec_outputs.extend(dec_output) <span class="comment"># [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]</span></span><br><span class="line">        <span class="comment"># A = [1, 2, 3]</span></span><br><span class="line">        <span class="comment"># B = [[&#x27;a&#x27;, &#x27;b&#x27;]]</span></span><br><span class="line">        <span class="comment"># A.extend([4])</span></span><br><span class="line">        <span class="comment"># A.extend([5, 6])</span></span><br><span class="line">        <span class="comment"># B.extend([&#x27;c&#x27;, &#x27;d&#x27;])</span></span><br><span class="line">        <span class="comment"># B.extend([[&#x27;e&#x27;, &#x27;f&#x27;]])</span></span><br><span class="line">        <span class="comment"># print(A)</span></span><br><span class="line">        <span class="comment"># print(B)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># result:</span></span><br><span class="line">        <span class="comment"># [1, 2, 3, 4, 5, 6]</span></span><br><span class="line">        <span class="comment"># [[&#x27;a&#x27;, &#x27;b&#x27;], &#x27;c&#x27;, &#x27;d&#x27;, [&#x27;e&#x27;, &#x27;f&#x27;]]</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(Data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义DataLoader&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_inputs, dec_inputs, dec_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataSet, self).__init__()</span><br><span class="line">        self.enc_inputs = enc_inputs</span><br><span class="line">        self.dec_inputs = dec_inputs</span><br><span class="line">        self.dec_outputs = dec_outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># data.DataLoader</span></span><br><span class="line"><span class="comment"># dataset：（数据类型 dataset）</span></span><br><span class="line"><span class="comment"># batch_size：（数据类型 int）</span></span><br><span class="line"><span class="comment"># shuffle：（数据类型 bool）洗牌。默认设置为False。在每次迭代训练时是否将数据洗牌，默认设置是False。将输入数据的顺序打乱，是为了使数据更有独立性，但如果数据是有序列特征的，就不要设置成True了。</span></span><br><span class="line"><span class="comment"># batch_sampler：（数据类型 Sampler）</span></span><br><span class="line"><span class="comment"># sampler：（数据类型 Sampler）</span></span><br><span class="line"><span class="comment"># num_workers：（数据类型 Int）</span></span><br><span class="line"><span class="comment"># pin_memory：（数据类型 bool）</span></span><br><span class="line"><span class="comment"># drop_last：（数据类型 bool）</span></span><br><span class="line"><span class="comment"># timeout：（数据类型 numeric）</span></span><br><span class="line"></span><br><span class="line">loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)  <span class="comment"># 2个batch是两句话</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================================================================================================</span></span><br><span class="line"><span class="comment"># Transformer模型(省去了Embedding层，因为我们之前自己定义过src_vocab和tgt_vocab)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):<span class="comment">#序列（句子），不够长时用pad填补。为了让pad的位置不参与权重计算，将pad=0的位置设为true</span></span><br><span class="line">    <span class="comment"># pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)</span></span><br><span class="line"><span class="string">    encoder和decoder都可能调用这个函数，所以seq_len视情况而定</span></span><br><span class="line"><span class="string">    seq_q: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_k: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_len could be src_len or it could be tgt_len</span></span><br><span class="line"><span class="string">    seq_len in seq_q and seq_len in seq_k maybe not equal</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size, len_q = seq_q.size()  <span class="comment"># 这个seq_q只是用来expand维度的</span></span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(zero) is PAD token</span></span><br><span class="line">    <span class="comment"># 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]</span></span><br><span class="line">    <span class="comment"># 判断是否为0，是0则为True，True则masked，并扩一个维度。# 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]，--&gt;[[F,F,F,F,T]][F,F,F,F,T]</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, len_k]</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):<span class="comment">#一次只翻译一个词</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;建议打印出来看看是什么的输出（一目了然）</span></span><br><span class="line"><span class="string">    seq: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># attn_shape: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)  <span class="comment"># 生成一个上三角矩阵</span></span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">        K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k)  <span class="comment"># scores : [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment"># mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素）</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># Fills elements of self tensor with value where mask is True.</span></span><br><span class="line"></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)  <span class="comment"># 对最后一个维度(v)做softmax</span></span><br><span class="line">        <span class="comment"># scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        context = torch.matmul(attn, V)  <span class="comment"># context: [batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="comment"># context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）</span></span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>deeplearning pytorch</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>deeplearning</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Self Attention</title>
    <url>/2022/07/31/Self-Attention/</url>
    <content><![CDATA[<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h3><h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><p><img src="/2022/07/31/Self-Attention/881659278403_.pic.jpg" alt="881659278403_.pic"></p>
<h4 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h4><p><img src="/2022/07/31/Self-Attention/30866925-c326e92942025a3a6c6b1e8b96ccc0ce.png" alt="30866925-c326e92942025a3a6c6b1e8b96ccc0ce"></p>
<h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><p><img src="/2022/07/31/Self-Attention/30867091-d28fe5868178a62d9faab212486b474b.png" alt="30867091-d28fe5868178a62d9faab212486b474b"></p>
<h3 id="Muiltihead-Self-Attention"><a href="#Muiltihead-Self-Attention" class="headerlink" title="Muiltihead Self Attention"></a>Muiltihead Self Attention</h3><h4 id="概览-1"><a href="#概览-1" class="headerlink" title="概览"></a>概览</h4><p><img src="/2022/07/31/Self-Attention/image-20220801014946983.png" alt="image-20220801014946983"></p>
<h4 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h4><p><img src="/2022/07/31/Self-Attention/30867156-2219b84d50e04f41dd60cbe13b2994ac.png" alt="30867156-2219b84d50e04f41dd60cbe13b2994ac"></p>
<ul>
<li>这里的 a i 和 a j 是同一个输入(可以理解为下图的X)</li>
</ul>
<h4 id="完整过程"><a href="#完整过程" class="headerlink" title="完整过程"></a>完整过程</h4><p><img src="/2022/07/31/Self-Attention/QQ20220801-015325.png" alt="QQ20220801-015325"></p>
<p><img src="/2022/07/31/Self-Attention/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2NjE0NjM2,size_16,color_FFFFFF,t_70-20220801020808429.png" alt="在这里插入图片描述"></p>
<ul>
<li><p>八个头相当于八个不同的表征子空间，类似于apple拥有水果的含义，同时也有商标的含义，不同的含义由不同的表征子空间学习。</p>
</li>
<li><p>让其他词的Q来和apple这个词不同组的K-V进行attention。</p>
</li>
<li><p>再把所有的attention结果拼接起来，通过一个全连接层（矩阵变换）得到最终结果。</p>
</li>
<li><p>X是一开始经过Embedding的词向量矩阵，R为之前层输出的，他俩都可以进行Multihead Self Attention</p>
</li>
</ul>
]]></content>
      <categories>
        <category>deeplearning</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>blog</title>
    <url>/2022/07/29/blog/</url>
    <content><![CDATA[<h3 id="Markdown基本操作"><a href="#Markdown基本操作" class="headerlink" title="Markdown基本操作"></a>Markdown基本操作</h3><ul>
<li>标题</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section"># A (一级标题)</span></span><br><span class="line"><span class="section">## A</span></span><br><span class="line"><span class="section">### A</span></span><br><span class="line"><span class="section">#### A</span></span><br><span class="line"><span class="section">##### A</span></span><br><span class="line"><span class="section">###### A （六级标题）</span></span><br></pre></td></tr></table></figure>



<ul>
<li>字体</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="emphasis">*斜体*</span></span><br><span class="line"><span class="strong">**粗体**</span></span><br><span class="line"><span class="strong">**<span class="emphasis">*斜粗体*</span>**</span></span><br></pre></td></tr></table></figure>



<ul>
<li>分割线、下划线</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="strong">**<span class="emphasis">*</span></span></span><br><span class="line"><span class="emphasis"><span class="strong"></span></span></span><br><span class="line"><span class="emphasis"><span class="strong"><span class="language-xml"><span class="tag">&lt;<span class="name">u</span>&gt;</span></span>带下划线文本<span class="language-xml"><span class="tag">&lt;/<span class="name">u</span>&gt;</span></span></span></span></span><br></pre></td></tr></table></figure>



<ul>
<li>脚注</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">[^鼠标移到我身上]：会弹出我！</span><br></pre></td></tr></table></figure>



<ul>
<li>列表</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="bullet">*</span> 第一项		</span><br><span class="line"><span class="bullet">*</span> 第二项		</span><br><span class="line"><span class="bullet">*</span> 第三项   </span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> 第一项		</span><br><span class="line"><span class="bullet">-</span> 第二项		</span><br><span class="line"><span class="bullet">-</span> 第三项  </span><br></pre></td></tr></table></figure>



<ul>
<li>最左边的竖线</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="quote">&gt; </span></span><br></pre></td></tr></table></figure>



<ul>
<li>代码</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="code">`print()`</span> </span><br><span class="line"></span><br><span class="line"><span class="code">```</span></span><br></pre></td></tr></table></figure>



<ul>
<li>链接</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">[<span class="string">链接名称</span>](<span class="link">链接地址</span>)</span><br><span class="line"></span><br><span class="line">&lt;链接地址&gt;</span><br></pre></td></tr></table></figure>



<ul>
<li>图片</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">![<span class="string">alt 代替文本</span>](<span class="link">图片地址</span>)</span><br><span class="line"></span><br><span class="line">![<span class="string">alt 代替文本</span>](<span class="link">图片地址 &quot;可选标题&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;http://static.runoob.com/images/runoob-logo.png&quot;</span> <span class="attr">width</span>=<span class="string">&quot;50%&quot;</span>&gt;</span></span></span><br></pre></td></tr></table></figure>



<ul>
<li>表格</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">|  表头   | 表头  |</span><br><span class="line">|  ----  | ----  |</span><br><span class="line">| 单元格  | 单元格 |</span><br><span class="line">| 单元格  | 单元格 |</span><br><span class="line"></span><br><span class="line">我们可以设置表格的对齐方式：</span><br><span class="line">-: 设置内容和标题栏居右对齐。</span><br><span class="line">:- 设置内容和标题栏居左对齐。</span><br><span class="line">:-: 设置内容和标题栏居中对齐。</span><br><span class="line"></span><br><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :----| ----: | :----: |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br></pre></td></tr></table></figure>



<h3 id="Hexo基本操作"><a href="#Hexo基本操作" class="headerlink" title="Hexo基本操作"></a>Hexo基本操作</h3><ul>
<li>发布文章</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;文章名&quot;</span></span><br></pre></td></tr></table></figure>

<p>去&#x2F;Users&#x2F;jiangangkong&#x2F;blog&#x2F;source&#x2F;_posts&#x2F;文章名.md 下写文章</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>



<h3 id="规范"><a href="#规范" class="headerlink" title="规范"></a>规范</h3><ul>
<li>每篇文章下最大的标题为三级标题</li>
<li>并列关系用 * </li>
<li>顺序关系用1. 2.</li>
</ul>
]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
